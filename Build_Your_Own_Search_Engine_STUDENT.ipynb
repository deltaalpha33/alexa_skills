{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own Search Engine\n",
    "\n",
    "The success of Google can be boiled down to this simple equation:\n",
    "\n",
    "    \"information retrieval\" + \"linear algebra / graph analytics\" + \"machine learning\" = $$$\n",
    "\n",
    "In a little more detail, these key ingredients are:\n",
    "\n",
    "- indexing and keyword search for organizing enormous collection of web documents\n",
    "- the PageRank algorithm for computing importance of web pages (for ranking search results)\n",
    "- learning to rerank results based on user clicks (and other features)\n",
    "\n",
    "We're going to work on part 1 of the equation by developing our own search engine for indexing and retrieving documents via boolean search queryies. That just leads to one dollar sign:\n",
    "\n",
    "    \"information retrieval\" = $\n",
    "\n",
    "But it should still be fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-533531986e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[1;31m# Packages which can be lazily imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\stem\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misri\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \"\"\"\n\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaintext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[1;32mclass\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCorpusReader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0mReader\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconsist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mplaintext\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mParagraphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\theda_000\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36mPlaintextCorpusReader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     def __init__(self, root, fileids,\n\u001b[1;32m     41\u001b[0m                  \u001b[0mword_tokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                  sent_tokenizer=nltk.data.LazyLoader(\n\u001b[0m\u001b[1;32m     43\u001b[0m                      'tokenizers/punkt/english.pickle'),\n\u001b[1;32m     44\u001b[0m                  \u001b[0mpara_block_reader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_blankline_block\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from operator import itemgetter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b6c768b519ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# run NLTK downloader to get Punkt sentence chunking model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# look for \"punkt\" under \"Models\" tab in NLTK Downloader gui that pops up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# run NLTK downloader to get Punkt sentence chunking model\n",
    "# look for \"punkt\" under \"Models\" tab in NLTK Downloader gui that pops up\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MySearchEngine class\n",
    "\n",
    "We're going to be creating a single class that maintains an in-memory index for adding, removing, and querying for documents. We'll assume that documents have an associated id that can be used to uniquely identify it (e.g., a URL for a web document). For this search engine, we'll be using the following definitions:\n",
    "\n",
    "- collection of documents: $D$\n",
    "- number of documents in collection: $N = \\lvert D \\rvert$\n",
    "- term frequency (number of times term $t$ appears in document $d$): $tf(t, d)$\n",
    "- document frequency of a term (number if documents that contain term $t$): $df(t, D) = \\lvert \\{d \\in D : t \\in d\\} \\rvert$\n",
    "- inverse document frequency: $idf(t, D) = \\log_{10} \\left( \\frac{N}{1 + df(t, D)} \\right)$\n",
    "- term frequency-inverse document frequency: $tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D)$\n",
    "- dot product between two documents: $d_1 \\cdot d_2 = \\sum_{t} tfidf(t, d_1, D) \\cdot tfidf(t, d_2, D)$\n",
    "- length of document: $\\lVert d \\rVert = \\sqrt{\\sum_{t} tfidf(t, d, D)}$\n",
    "- cosine similarity between two documents: $sim(d_1, d_2) = \\frac{d_1 \\cdot d_2}{\\lVert d_1 \\rVert \\cdot \\lVert d_2 \\rVert}$\n",
    "\n",
    "Note: This search engine will be dynamic in the sense that documents can be added and removed at any time. One consequence is that we won't be precomputing $tfidf(t, d)$ for terms in a document $d$ because $idf(t, D)$ will change as the document collection changes. Instead, we'll just be maintaining $tf(t, d)$ values for documents, and multiplying by the current value of $idf(t, D)$ as needed (according to definitions above).\n",
    "\n",
    "A second consequence is that we won't be storing the term frequencies for a document as a fixed length vector, again, because the vocabulary will be changing as documents get added or removed. Instead, we'll be storing term frequencies as Counter objects (dictionaries). This is like using a sparse vector to represent the term frequencies (but in a position-independent way). Now computing the dot product will involve finding which terms occur in both documents, since only those will affect the dot product. (If a term is missing from one of the documents, the product of the two counts will be zero.)\n",
    "\n",
    "Tip: Try completing this notebook in the style of \"test-driven development\" (see https://en.wikipedia.org/wiki/Test-driven_development). That is, just implement enough of the methods in the class to get the initial tests to pass, then implement some more to get the next set of tests to pass, and so on. These aren't actual PyUnit unit tests, but it's a similar idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'is', 'the', 'u.s.', 'located', 'i', 'have', 'no', 'idea']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strindex(text, val):\n",
    "    if (text+val).index(val) == len(text):\n",
    "        return -1\n",
    "    return text.index(val)\n",
    "text = \"Where is the U.S. located? I have no idea.\"\n",
    "[y.lower() for y in nltk.word_tokenize(text) if strindex(string.punctuation,y) == -1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySearchEngine():\n",
    "    def __init__(self):\n",
    "        # Dict[str, str]: maps document id to original/raw text\n",
    "        self.raw_text = {}\n",
    "        \n",
    "        # Dict[str, Counter]: maps document id to term vector (counts of terms in document)\n",
    "        self.term_vectors = {}\n",
    "        \n",
    "        # Counter: maps term to count of how many documents contain term\n",
    "        self.doc_freq = Counter()\n",
    "        \n",
    "        # Dict[str, set]: maps term to set of ids of documents that contain term\n",
    "        self.inverted_index = defaultdict(set)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    #  indexing\n",
    "    # ------------------------------------------------------------------------\n",
    "    def strindex(self, text, val):\n",
    "        if (text+val).index(val) == len(text):\n",
    "            return -1\n",
    "        return text.index(val)\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Converts text into tokens (also called \"terms\" or \"words\").\n",
    "        \n",
    "            This function should also handle normalization, e.g., lowercasing and \n",
    "            removing punctuation.\n",
    "        \n",
    "            For example, \"The cat in the hat.\" --> [\"the\", \"cat\", \"in\", \"the\", \"hat\"]\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            text: str\n",
    "                The string to separate into tokens.\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            list(str)\n",
    "                A list where each element is a token.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Hint: use NLTK's recommended word_tokenize() then filter out punctuation\n",
    "        # It uses Punkt for sentence splitting and then tokenizes each sentence.\n",
    "        # You'll notice that it's able to differentiate between an end-of-sentence period \n",
    "        # versus a period that's part of an abbreviation (like \"U.S.\").\n",
    "        \n",
    "        # tokenize\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        # lowercase and filter out punctuation (using string.punctuation)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        return [y.lower() for y in nltk.word_tokenize(text) if self.strindex(string.punctuation,y) == -1] \n",
    "\n",
    "\n",
    "    def add(self, id, text):\n",
    "        \"\"\" Adds document to index.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                A unique identifier for the document to add, e.g., the URL of a webpage.\n",
    "            text: str\n",
    "                The text of the document to be indexed.\n",
    "        \"\"\"\n",
    "        \n",
    "        if id in self.raw_text:\n",
    "            raise LookupError\n",
    "\n",
    "        self.raw_text[id] = text\n",
    "        \n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        c = Counter(tokens)\n",
    "        \n",
    "        self.term_vectors[id] = c\n",
    "        \n",
    "        for vv in c:\n",
    "            self.inverted_index[vv].update([id])\n",
    "        \n",
    "\n",
    "        storage = set()\n",
    "        storage.update( c.elements() )\n",
    "        for varK in storage:\n",
    "            self.doc_freq[varK] += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "    def remove(self, id):\n",
    "        \"\"\" Removes document from index.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                The identifier of the document to remove from the index.\n",
    "        \"\"\"\n",
    "        # check if document exists and throw exception if so\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        # remove raw text for this document\n",
    "        del self.raw_text[id]\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        #self.raw_text[id] = text\n",
    "        \n",
    "\n",
    "        c = self.term_vectors[id]\n",
    "        \n",
    "        for vv in self.term_vectors[id]:\n",
    "            self.inverted_index[vv] -= set([id])\n",
    "        \n",
    "\n",
    "        storage = set()\n",
    "        storage.update( c.elements() )\n",
    "        for varK in storage:\n",
    "            self.doc_freq[varK] -= 1\n",
    "            \n",
    "        # update document frequencies for terms found in this doc\n",
    "        # i.e., counts should decrease by 1 for each (unique) term in term vector\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        # remove term vector for this doc\n",
    "        del self.term_vectors[id]\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def get(self, id):\n",
    "        \"\"\" Returns the original (raw) text of a document.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            id: str\n",
    "                The identifier of the document to return.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.raw_text[id]\n",
    "        # check if document exists and throw exception if so\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        # return raw text\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def num_docs(self):\n",
    "        \"\"\" Returns the current number of documents in index. \n",
    "        \"\"\"\n",
    "        return len(self.raw_text)\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    #  matching\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def get_matches_term(self, term):\n",
    "        \"\"\" Returns ids of documents that contain term.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            term: str\n",
    "                A single token, e.g., \"cat\" to match on.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain term.\n",
    "        \"\"\"\n",
    "        term = term.lower()\n",
    "        \n",
    "        return self.inverted_index[term]\n",
    "        \n",
    "        # NOTE: term needs to be lowercased so can match output of tokenizer\n",
    "        # look up term in inverted index\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def get_matches_OR(self, terms):\n",
    "        \"\"\" Returns set of documents that contain at least one of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to match on, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain at least one of the term.\n",
    "        \"\"\"\n",
    "        s = set()\n",
    "        for term in terms:\n",
    "            t = term.lower()\n",
    "            s.update(self.inverted_index[t])\n",
    "        return s\n",
    "        # initialize set of ids to empty set\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        # union ids with sets of ids matching any of the terms\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def get_matches_AND(self, terms):\n",
    "        \"\"\" Returns set of documents that contain all of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to match on, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that contain each term.\n",
    "        \"\"\" \n",
    "        s = set()\n",
    "        s.update(self.inverted_index[terms[0]])\n",
    "        for term in terms:\n",
    "            t = term.lower()\n",
    "            s &= (self.inverted_index[t])\n",
    "        return s\n",
    "        # initialize set of ids to those that match first term\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        # intersect with sets of ids matching rest of terms\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def get_matches_NOT(self, terms):\n",
    "        \"\"\" Returns set of documents that don't contain any of the specified terms.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            terms: iterable(str)\n",
    "                An iterable of terms to avoid, e.g., [\"cat\", \"hat\"].\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            set(str)\n",
    "                A set of ids of documents that don't contain any of the terms.\n",
    "        \"\"\"\n",
    "        ids = set(self.raw_text.keys())\n",
    "        #print(ids)\n",
    "        for term in terms:\n",
    "            #print(set(self.inverted_index[term.lower()]))\n",
    "            ids -= set(self.inverted_index[term.lower()])\n",
    "        return ids\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    #  scoring\n",
    "    # ------------------------------------------------------------------------\n",
    "        \n",
    "    def idf(self, term):\n",
    "        \"\"\" Returns current inverse document frequency weight for a specified term.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            term: str\n",
    "                A term.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The value idf(t, D) as defined above.\n",
    "        \"\"\" \n",
    "        N = len(self.raw_text.keys())\n",
    "        n = self.doc_freq[term]\n",
    "        \n",
    "        return np.log10(N/(1+n))\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def dot_product(self, tv1, tv2):\n",
    "        \"\"\" Returns dot product between two term vectors (including idf weighting).\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            tv1: Counter\n",
    "                A Counter that contains term frequencies for terms in document 1.\n",
    "            tv2: Counter\n",
    "                A Counter that contains term frequencies for terms in document 2.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The dot product of documents 1 and 2 as defined above.\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for t in tv1:\n",
    "            total += self.idf(t) * tv1[t]*tv2[t] * self.idf(t)\n",
    "        \n",
    "        return total\n",
    "        # iterate over terms of one document\n",
    "        # if term is also in other document, \n",
    "        # then add their product (tfidf(t,d1) * tfidf(t,d2)) to a running total\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def length(self, tv):\n",
    "        \"\"\" Returns the length of a document (including idf weighting).\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            tv: Counter\n",
    "                A Counter that contains term frequencies for terms in the document.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The length of the document as defined above.\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for t in tv:\n",
    "            total += (tv[t]*self.idf(t))**2\n",
    "            \n",
    "        return total ** 0.5\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "    def cosine_similarity(self, tv1, tv2):\n",
    "        \"\"\" Returns the cosine similarity (including idf weighting).\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            tv1: Counter\n",
    "                A Counter that contains term frequencies for terms in document 1.\n",
    "            tv2: Counter\n",
    "                A Counter that contains term frequencies for terms in document 2.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                The cosine similarity of documents 1 and 2 as defined above.\n",
    "        \"\"\"\n",
    "        dot = self.dot_product(tv1,tv2)\n",
    "        mags = self.length(tv1) * self.length(tv2)\n",
    "        \n",
    "        return dot/mags\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    #  querying\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def query(self, q, k=10):\n",
    "        \"\"\" Returns up to top k documents matching at least one term in query q, sorted by relevance.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            q: str\n",
    "                A string containing words to match on, e.g., \"cat hat\".\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            List(tuple(str, float))\n",
    "                A list of (document, score) pairs sorted in descending order.\n",
    "                \n",
    "        \"\"\"\n",
    "        # tokenize query\n",
    "        tokens = self.tokenize(q)\n",
    "\n",
    "        # get matches (just support OR style queries for now...)\n",
    "        matches = self.get_matches_OR(tokens)\n",
    "                \n",
    "        # convert query to a term vector (Counter over tokens)\n",
    "        c = Counter(tokens)\n",
    "        \n",
    "        scores = []\n",
    "        # score each match by computing cosine similarity between query and document\n",
    "        for m in matches:\n",
    "            scores.append( ( m, self.cosine_similarity(c,self.term_vectors[m]) ) )\n",
    "        ### YOUR CODE HERE ###\n",
    "        scores.sort(key=itemgetter(1))\n",
    "        scores = scores[::-1]\n",
    "        # sort results and return top k\n",
    "        return scores[:k]\n",
    "        ### YOUR CODE HERE ###\n",
    "    def advanced_query(self, q, k=10,require=None,exclude=None):\n",
    "        \"\"\" Returns up to top k documents matching at least one term in query q, sorted by relevance.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            q: str\n",
    "                A string containing words to match on, e.g., \"cat hat\".\n",
    "                +word means require word\n",
    "                -word means require not having word\n",
    "                \n",
    "            \n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            List(tuple(str, float))\n",
    "                A list of (document, score) pairs sorted in descending order.\n",
    "                \n",
    "        \"\"\"\n",
    "        doAnd = False\n",
    "        doNot = False\n",
    "        if require != None:\n",
    "            ands = self.get_matches_AND( self.tokenize(require) )\n",
    "            doAnd = True\n",
    "        if exclude != None:\n",
    "            nots = self.get_matches_NOT( self.tokenize(exclude) )\n",
    "            doNot = True\n",
    "        # tokenize query\n",
    "        tokens = self.tokenize(q)\n",
    "\n",
    "        # get matches (just support OR style queries for now...)\n",
    "        matches = self.get_matches_OR(tokens)\n",
    "                \n",
    "        if doAnd:\n",
    "            fix_matches = (matches & ands) \n",
    "        if doNot:\n",
    "            fix_matches = (matches & nots)\n",
    "        # convert query to a term vector (Counter over tokens)\n",
    "        c = Counter(tokens)\n",
    "        \n",
    "        scores = []\n",
    "        # score each match by computing cosine similarity between query and document\n",
    "        for m in fix_matches:\n",
    "            scores.append( ( m, self.cosine_similarity(c,self.term_vectors[m]) ) )\n",
    "        ### YOUR CODE HERE ###\n",
    "        scores.sort(key=itemgetter(1))\n",
    "        scores = scores[::-1]\n",
    "        # sort results and return top k\n",
    "        return scores[:k]\n",
    "        ### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Adding documents\n",
    "\n",
    "This section contains examples of adding documents and retrieving their raw text by id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0 Initialize search engine and add sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5e04161eb1d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMySearchEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"The cat in the hat.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Which cat wrote the cat book?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"A dog is wearing a hat and reading a book!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Her day started off with a good book.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f319a451190c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[1;31m# Counter: maps term to count of how many documents contain term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;31m# Dict[str, set]: maps term to set of ids of documents that contain term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "engine = MySearchEngine()\n",
    "engine.add(\"d1\", \"The cat in the hat.\")\n",
    "engine.add(\"d2\", \"Which cat wrote the cat book?\")\n",
    "engine.add(\"d3\", \"A dog is wearing a hat and reading a book!\")\n",
    "engine.add(\"d4\", \"Her day started off with a good book.\")\n",
    "engine.add(\"d5\", \"My days are numbered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Verify that document count is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.num_docs()\n",
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Verify that retrieving document \"d1\" (by id) returns the string: \"The cat in the hat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'d6'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8228c59cd888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d6\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e5bdd38fa2be>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[1;31m# check if document exists and throw exception if so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'd6'"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.get(\"d6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Verify that adding a document with an id that's already been used throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4193bd704527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"The cat in the hat.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e5bdd38fa2be>\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, id, text)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.add(\"d1\", \"The cat in the hat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Verify that removing a document by id reduces the number of documents by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.remove('d1')\n",
    "engine.num_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Verify that trying to retrieve that document again (by id) throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'d1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6ccf8872426d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e5bdd38fa2be>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[1;31m# check if document exists and throw exception if so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'd1'"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.get('d1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Verify that we can add document d5 (\"My days are numbered.\") back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "engine.add(\"d1\", \"The cat in the hat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Verify tokenization by tokenizing the string \"The U.S. was represented at the meeting.\" This should yield:\n",
    "\n",
    "```python\n",
    "['the', 'u.s.', 'was', 'represented', 'at', 'the', 'meeting']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'u.s.', 'was', 'represented', 'at', 'the', 'meeting']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.tokenize(\"The U.S. was represented at the meeting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Matching documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Verify that matching on single term \"cat\" returns `{'d1', 'd2'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1', 'd2'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.get_matches_term(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Verify that matching on \"cat\" or \"dog\" returns `{'d1', 'd2', 'd3'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1', 'd2', 'd3'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.get_matches_OR([\"cat\",\"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Verify that matching on \"cat\" and \"hat\" returns `{'d1'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.get_matches_AND([\"cat\",\"hat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Verify that matching documents that do NOT contain \"cat\" or \"dog\" returns `{'d4', 'd5'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d4', 'd5'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.get_matches_NOT([\"cat\",\"hat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Query result scoring and ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Verify that the idf of \"cat\" is `0.221848749616` and the idf of \"book\" is `0.0969100130081` (assuming d1, ..., d5 from above). Does it make sense that the weight for \"cat\" is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22184874961635639"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "engine.idf(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4.2 Verify that a query for \"cat\" returns: \n",
    "```python\n",
    "[('d2', 0.58656677583588812), \n",
    " ('d1', 0.32937676397011012)]\n",
    "```\n",
    "Why is d2 ranked higher than d1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d2', 0.58656677583588812), ('d1', 0.32937676397011012)]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "print(engine.query(\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Verify that a query for \"cat book\" returns:\n",
    "```python\n",
    "[('d2', 0.58880446883493398),\n",
    " ('d1', 0.30183523984479854),\n",
    " ('d4', 0.038624814550947489),\n",
    " ('d3', 0.034111490780396964)]\n",
    "```\n",
    "Does this ordering make sense? Think about how idf and also the length of documents affect scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d2', 0.58880446883493398), ('d1', 0.30183523984479854), ('d4', 0.038624814550947489), ('d3', 0.034111490780396964)]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "print(engine.query(\"cat book\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d3', 0.31666912632857153), ('d1', 0.15687561261972546)]\n"
     ]
    }
   ],
   "source": [
    "print(engine.advanced_query(\"dog cat book\",require=\"hat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-019a5d854d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dog cat\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"book\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "print(engine.advanced_query(\"dog cat\",exclude=\"book\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Possible ideas for extra time:\n",
    "\n",
    "- implement a query language that allows `\"+\"` to mean \"must have\" and `\"-\"` to mean \"must not have\"; e.g., `\"cat -book\"` would return all documents that contain cat but not book\n",
    "- incorporate stemming (or lemmatization) into tokenization (see NLTK documentation)\n",
    "- incorporate stopword removal\n",
    "- think about how phrase search could be supported (i.e., require that terms in a quoted phrase  appear next to each other in matching documents, e.g., like \"burger king\")\n",
    "- ingest a set of documents (e.g., song lyrics) and see if ranked results look good for various queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
